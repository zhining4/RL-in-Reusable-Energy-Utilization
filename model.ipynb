{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee84a61",
   "metadata": {},
   "source": [
    "## Q-learning for washing machine (Yunzhe and Weisheng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6655c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decimal import Decimal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaedf681",
   "metadata": {},
   "source": [
    "### State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c46704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the states\n",
    "def callspace(lower,upper,interval,decimal):\n",
    "    states = {}\n",
    "    for i in range (24):\n",
    "        consumption = np.arange(lower, upper, interval)\n",
    "        for each in consumption:\n",
    "            if i >= 12:\n",
    "                key = \"(\" + str((i)) + \" PM, \" + str(round(each,decimal)) + \")\"\n",
    "                states[key] = [i, round(each,decimal)]\n",
    "            else:\n",
    "                key = \"(\" + str(i) + \" AM, \" + str(round(each,decimal)) + \")\"\n",
    "                states[key] = [i, round(each,decimal)]\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7975eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_to_state = callspace(0.00,1.28,0.01,2)\n",
    "#time_to_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb96dde",
   "metadata": {},
   "source": [
    "### Action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3543aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 defined for turn off and 1 defined for turn on\n",
    "actions = [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565b2dd",
   "metadata": {},
   "source": [
    "### Reward formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3838a4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 2)\n"
     ]
    }
   ],
   "source": [
    "# Q tabel \n",
    "def createQ(total):\n",
    "    return np.array(np.zeros([total,2]))\n",
    "\n",
    "Q = createQ(len(time_to_state))\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9463e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_time = dict((tuple(state),time) for time,state in time_to_state.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5964282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.75 # Discount factor \n",
    "alpha = 0.9 # Learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f0dd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    # Initialize alpha, gamma, states, actions, rewards, and Q-values\n",
    "    def __init__(self, alpha, gamma, time_to_state, actions, state_to_time, Q, price, energy_cost, penalty, t_prefer_s, t_prefer_e, max_limit, interval, lower,upper,decimal):\n",
    "        \n",
    "        self.gamma = gamma  \n",
    "        self.alpha = alpha \n",
    "        self.time_to_state = time_to_state\n",
    "        self.actions = actions\n",
    "        self.state_to_time = state_to_time\n",
    "        self.Q = Q\n",
    "        self.price = price\n",
    "        self.energy_cost = energy_cost\n",
    "        self.penalty = penalty\n",
    "        self.t_prefer_s = t_prefer_s\n",
    "        self.t_prefer_e = t_prefer_e\n",
    "        self.max_limit = max_limit\n",
    "        self.interval = interval\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.decimal = decimal\n",
    "        \n",
    "    def calReward(self, current_state, time, energy):\n",
    "        fix_cost = self.price[time] * energy\n",
    "        if energy > self.max_limit:\n",
    "            if time < self.t_prefer_s:\n",
    "                return -(fix_cost + self.penalty * (self.t_prefer_s - time) + 999*(energy - self.max_limit))\n",
    "            elif time > self.t_prefer_e:\n",
    "                return -(fix_cost + self.penalty * (time - self.t_prefer_e)+ 999*(energy - self.max_limit))\n",
    "            else:\n",
    "                return -(fix_cost + 999*(energy - self.max_limit)) \n",
    "        else:\n",
    "            if time < self.t_prefer_s:\n",
    "                return -(fix_cost + self.penalty * (self.t_prefer_s - time))\n",
    "            elif time > self.t_prefer_e:\n",
    "                return -(fix_cost + self.penalty * (time - self.t_prefer_e))\n",
    "            else:\n",
    "                return -(fix_cost) \n",
    "        \n",
    "        \n",
    "        \n",
    "    def training (self, start_state, end_state,iterations):\n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.randint(0,self.Q.shape[0]) \n",
    "            playable_actions = [0,1]\n",
    "            time = int(current_state / self.interval)\n",
    "            consumption = (current_state % self.interval) * self.decimal + self.lower\n",
    "            next_action = np.random.choice(playable_actions)\n",
    "            if time >= 22:\n",
    "                next_state =  current_state % self.interval\n",
    "            else:\n",
    "                next_state = current_state +  self.interval\n",
    "            reward = self.calReward(next_state, time, consumption)\n",
    "            TD = reward + self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state,next_action]\n",
    "            self.Q[current_state,next_action] += self.alpha * TD\n",
    "\n",
    "        self.get_optimal_action(start_state, end_state)\n",
    "        \n",
    "    def print_welcome(self, idx):\n",
    "        if idx == 0:\n",
    "            print(\"------------------------------------\")\n",
    "            print(\"|        WELCOME TO Q-Learning        |\")\n",
    "            print(\"------------------------------------\")\n",
    "        elif idx == 1:\n",
    "            print(\"t -     STATE  -  ACTION\")\n",
    "            print(\"================================\")\n",
    "        \n",
    "    def get_optimal_action(self,start_state, end_state):\n",
    "        route = []\n",
    "        self.print_welcome(1)\n",
    "        count = 0\n",
    "        \n",
    "        start_point = time_to_state[start_state][0]\n",
    "        end_point = time_to_state[end_state][0]\n",
    "        current_state = time_to_state[start_state]\n",
    "        while(start_point != end_point):\n",
    "            #print(((self.upper - self.lower)/self.interval))\n",
    "            #print(current_state)\n",
    "            rows = int(current_state[0]*self.interval + (current_state[1] - self.lower) / ((self.upper - self.lower)/(self.interval - 1)))\n",
    "            #print(rows)\n",
    "            next_action = np.argmax(self.Q[rows,])\n",
    "            names_state = state_to_time[tuple(current_state)]\n",
    "            if next_action == 0:\n",
    "                print(count,\" -\", names_state, \"Turn Off\")\n",
    "            else:\n",
    "                print(count,\" -\", names_state, \"Turn On\")\n",
    "            \n",
    "            current_state = [current_state[0] + 1, self.energy_cost[start_point + 1]]\n",
    "            start_point += 1\n",
    "            count += 1\n",
    "        #print(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c8a3520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "|        WELCOME TO Q-Learning        |\n",
      "------------------------------------\n",
      "t -     STATE  -  ACTION\n",
      "================================\n",
      "0  - (0 AM, 0.0) Turn On\n",
      "1  - (1 AM, 0.0) Turn On\n",
      "2  - (2 AM, 0.0) Turn On\n",
      "3  - (3 AM, 0.0) Turn Off\n",
      "4  - (4 AM, 0.0) Turn On\n",
      "5  - (5 AM, 0.0) Turn On\n",
      "6  - (6 AM, 0.0) Turn Off\n",
      "7  - (7 AM, 0.0) Turn Off\n",
      "8  - (8 AM, 0.0) Turn Off\n",
      "9  - (9 AM, 0.0) Turn Off\n",
      "10  - (10 AM, 0.5) Turn Off\n",
      "11  - (11 AM, 0.16) Turn Off\n",
      "12  - (12 PM, 0.54) Turn Off\n",
      "13  - (13 PM, 0.13) Turn On\n",
      "14  - (14 PM, 0.0) Turn On\n",
      "15  - (15 PM, 0.0) Turn On\n",
      "16  - (16 PM, 0.0) Turn On\n",
      "17  - (17 PM, 0.0) Turn On\n",
      "18  - (18 PM, 0.0) Turn On\n",
      "19  - (19 PM, 0.0) Turn On\n",
      "20  - (20 PM, 0.0) Turn Off\n",
      "21  - (21 PM, 0.0) Turn Off\n",
      "22  - (22 PM, 0.0) Turn On\n"
     ]
    }
   ],
   "source": [
    "price = [1.23, 2.34,1.45,3.42,2.476,3.21,1.77,2.31,2.35,3.22,1.45,3.23,3.56,2,65,2,78,4.11,4.23,2,21,2.18,2.19,2.78]\n",
    "energy_cost = [0.0,0.0,0.0,0.0, 0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.16,0.54,0.13,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00]\n",
    "penalty = 0.7\n",
    "t_prefer_s = 11\n",
    "t_prefer_e = 13\n",
    "qagent = QAgent(alpha, gamma, time_to_state, actions, state_to_time, Q, price, energy_cost, penalty, t_prefer_s, t_prefer_e, 555, 129, 0.00,1.28,0.01)\n",
    "qagent.print_welcome(0)\n",
    "qagent.training('(0 AM, 0.0)', '(23 PM, 0.0)', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4303b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88755c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfb805",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0c962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b111cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
