# -*- coding: utf-8 -*-
"""WM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LERPLt0onETt4Y002RgnRfpfy56BFUir
"""

import random
import gym
from gym import spaces
import pandas as pd
import numpy as np
import datetime
import random


class WMEnv(gym.Env):
    def __init__(self, df, t_prefer_s, t_prefer_e, max_limit, penalty_time, penalty_energy, penalty_ope, mode):   
        super(WMEnv, self).__init__()
        self.df = df
        self.t_prefer_s = t_prefer_s
        self.t_prefer_e = t_prefer_e
        self.max_limit = max_limit # set a preferable upper bound of total energy consumption for all appliances in a day
        self.penalty_time = penalty_time # Penalty coefficient of running WM outside the preferable time range
        self.penalty_energy = penalty_energy # Penalty coefficient for exceeding the max_limit
        self.penalty_ope = penalty_ope # Penalty coefficient of exceeding the preferable number of time using the WM in a day
        self.prefer_ope = 1 # We prefer to use WM once a day
        self.observation_space = spaces.Box(low = 0, high = float('inf'), shape = (6,), dtype=np.float32)
        self.action_space = spaces.Discrete(2)        
        self.date_list = df.date.unique()
        
        self.mode = mode
        assert self.mode in ['training', 'testing'], 'mode should be either \'training\' or \'testing\''
        if self.mode == 'testing':
            # set counter to traverse all date in testing set
            self.cnt = 0
        
       
    def action_mapping(self, action):
        mapping = {0: "Turn off", 1: "Turn on"}
        return mapping[action]


    def step(self, action):
        if action == 0:
            # turn off, but still running WM in low power
            # Need to do EDA and decide the average energy consumption of WM running in low power
            self.cur_energy = np.random.uniform(0,0.1)
        else:
            # turn on
            # Need to do EDA and decide the average energy consumption of a washing cycle (1hour)
            self.cur_energy = np.random.uniform(1,1.5)
            self.cur_ope += 1 # count on current total number of operations(turn on) in a day
            
        # total energy_cost for wm
        self.total_energy_cost += self.cur_energy
        
        # compute net_energy_cost and total_net_energy_cost for all appliances at time t
        net_energy_cost = self.fixed_cost + self.cur_energy - self.generation        
        if net_energy_cost < 0:
            net_energy_cost = 0
        # compute cumulative sum of net_energy_cost
        self.total_net_energy_cost += net_energy_cost
        
        self.cur_reward = self.calReward(action, net_energy_cost)


        #Calculating elements in new state
        self.time += 1

        if self.time == 24:
            # if time = 24, we need to update using values at time = 0 in the following day.
            if self.mode == 'training':
                self.price = self.df[(self.df.date == datetime.datetime.strftime(pd.to_datetime(self.cur_date) + datetime.timedelta(days=1), '%Y-%m-%d')) & (self.df.t == 0)]['price'].values[0]
                self.generation = self.df[(self.df.date == datetime.datetime.strftime(pd.to_datetime(self.cur_date) + datetime.timedelta(days=1), '%Y-%m-%d')) & (self.df.t == 0)]['generation'].values[0]
                self.fixed_cost = self.df[(self.df.date == datetime.datetime.strftime(pd.to_datetime(self.cur_date) + datetime.timedelta(days=1), '%Y-%m-%d')) & (self.df.t == 0)]['fixed'].values[0]
                self.done = True
            else:
                self.done = True
        else:
            self.price = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time)]['price'].values[0]
            self.generation = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time)]['generation'].values[0]
            self.fixed_cost = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time)]['fixed'].values[0]
            
        self.state = [self.time, self.price, self.generation, self.fixed_cost, self.total_net_energy_cost, self.cur_ope]
        return self.state, self.cur_reward, self.done, {}


    def calReward(self, action, net_energy_cost):
    # calculating reward based on cur_energy and total_energy_cost
        net_energy_cost_reward = self.price * net_energy_cost
        
        if action == 1:        
            if self.total_net_energy_cost > self.max_limit:
                if self.time < self.t_prefer_s:
                    return -(net_energy_cost_reward + self.penalty_time * (self.t_prefer_s - self.time) + self.penalty_energy * (self.total_net_energy_cost - self.max_limit) + self.penalty_ope * abs((self.cur_ope - self.prefer_ope)))
                elif self.time > self.t_prefer_e:
                    return -(net_energy_cost_reward + self.penalty_time * (self.time - self.t_prefer_e) + self.penalty_energy * (self.total_net_energy_cost - self.max_limit) + self.penalty_ope * abs((self.cur_ope - self.prefer_ope)))
                else:
                    return -(net_energy_cost_reward + self.penalty_energy * (self.total_net_energy_cost - self.max_limit) + self.penalty_ope * abs((self.cur_ope - self.prefer_ope))) 
            else:
                if self.time < self.t_prefer_s:
                    return -(net_energy_cost_reward + self.penalty_time * (self.t_prefer_s - self.time) + self.penalty_ope * abs((self.cur_ope - self.prefer_ope)))
                elif self.time > self.t_prefer_e:
                    return -(net_energy_cost_reward + self.penalty_time * (self.time - self.t_prefer_e) + self.penalty_ope * abs((self.cur_ope - self.prefer_ope)))
                else:
                    return -(net_energy_cost_reward + self.penalty_ope * abs((self.cur_ope - self.prefer_ope)))
        else:
            if self.total_net_energy_cost > self.max_limit:
                return -(net_energy_cost_reward + self.penalty_energy * (self.total_net_energy_cost - self.max_limit) + self.penalty_ope * abs((self.cur_ope - self.prefer_ope))) 
            else:
                return -(net_energy_cost_reward + self.penalty_ope * abs((self.cur_ope - self.prefer_ope)))
    

    def reset(self):
        # Reset the state of the environment to an initial state
#        self.total_reward = 0
        self.done = False # flag of completion of current episode
        self.cur_reward = 0
        self.time = 0
        self.cur_energy = 0
        self.total_energy_cost = 0
        self.cur_ope = 0

        self.total_net_energy_cost = 0 # cumulative sum of net_energy_cost
        
#         self.cur_date = self.random_date(n=1) # in each episocde, randomly select a new day for interaction
        if self.mode == 'training':
            while True:
                sample = self.date_list[random.sample(range(len(self.date_list)), 1)][0] # sample a new day
                if datetime.datetime.strftime(pd.to_datetime(sample) + datetime.timedelta(days=1), '%Y-%m-%d') in self.date_list:
                    # check if the next day is in the dataset
                    self.cur_date = sample
                    break
        else:
            self.cur_date = self.date_list[self.cnt]
            self.cnt += 1

        self.fixed_cost = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time)]['fixed'].values[0]
        self.generation = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time)]['generation'].values[0]
        self.price = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time)]['price'].values[0]
        
        self.state = [self.time, self.price, self.generation, self.fixed_cost, self.total_net_energy_cost, self.cur_ope]
  
        return np.array(self.state, dtype=np.float32)

    def render(self, action):
        print('\n')
        if self.time - 1 >= 12:
            print(f'Time: {str(self.time - 1) + " PM"}')
        else:
            print(f'Time: {str(self.time - 1) + " AM"}')
        p = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time - 1)]['price'].values[0]
        g = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time - 1)]['generation'].values[0]
        c = self.df[(self.df.date == self.cur_date) & (self.df.t == self.time - 1)]['fixed'].values[0]
        print(f'Price: {p:.3f}')
        print(f'Generation:{g:.3f}')
        print(f'Fixed_cost for all other appliances:{c:.3f}')
        print(f'Action: {self.action_mapping(action)}')
        print(f'Current energy cost of WM:{self.cur_energy:.3f}')
        print(f'Total Energy Cost of WM: {self.total_energy_cost:.3f}') 
        print(f'Current net energy cost:{self.net_energy_cost:.3f}')
        print(f'Total net energy cost:{self.total_net_energy_cost:.3f}')
                                                                  
        print(f'Reward: {self.cur_reward:.3f}')

  
    def random_date(self, n, seed=None):
        start = pd.to_datetime(self.df.date).min()
        end = pd.to_datetime(self.df.date).max() + datetime.timedelta(days=-1) # remove the last day
        ndays = (end - start).days + 1
        return datetime.datetime.strftime((pd.to_timedelta(np.random.rand(n) * ndays, unit='D') + start)[0], '%Y-%m-%d')
        

